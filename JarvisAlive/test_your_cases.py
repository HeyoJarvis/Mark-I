#!/usr/bin/env python3
"""
Test Your Specific Use Cases

This tests the semantic architecture with your actual business scenarios
to prove it works for your real workloads.
"""

import asyncio
import sys
import os
from datetime import datetime
from dotenv import load_dotenv

# Load environment
load_dotenv()

# Add current directory to path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from orchestration.semantic_integration import SemanticJarvis
from ai_engines.anthropic_engine import AnthropicEngine
from ai_engines.base_engine import AIEngineConfig


async def test_your_business_cases():
    """Test semantic architecture with your actual business scenarios."""
    print("üéØ Testing Your Business Use Cases")
    print("=" * 50)
    
    # Check API key
    api_key = os.getenv('ANTHROPIC_API_KEY')
    if not api_key:
        print("‚ùå No API key found. Using mock responses.")
        return await test_with_mock()
    
    try:
        # Initialize with your real AI
        config = AIEngineConfig(
            api_key=api_key,
            model="claude-3-5-sonnet-20241022"
        )
        ai_engine = AnthropicEngine(config)
        jarvis = SemanticJarvis(ai_engine)
        
        # Your actual business test cases
        business_cases = [
            {
                "name": "üé® Logo Creation (Previously Problematic)",
                "request": "Create a professional logo for my artisan coffee roastery called 'Bean Craft Co' - I want something modern but with handcrafted feel",
                "expected_agent": "logo_generation_agent",
                "expected_capability": "logo_generation"
            },
            {
                "name": "üìä Market Research (Previously Problematic)", 
                "request": "I need comprehensive market analysis for electric vehicle charging stations in suburban office parks - focus on demand and competition",
                "expected_agent": "market_research_agent",
                "expected_capability": "market_analysis"
            },
            {
                "name": "üè¢ Complete Brand Package",
                "request": "Create complete branding for my sustainable fashion startup: brand strategy, logo, market research, and website - target eco-conscious millennials",
                "expected_strategy": "sequential_multi",
                "expected_agents": ["branding_agent", "logo_generation_agent", "market_research_agent"]
            },
            {
                "name": "üöÄ B2B SaaS Launch",
                "request": "Help me launch my project management SaaS: competitor analysis, brand identity, sales materials, and landing page - targeting small agencies",
                "expected_strategy": "parallel_multi",
                "expected_agents": ["market_research_agent", "branding_agent", "website_generator_agent"]
            }
        ]
        
        results = []
        
        for i, case in enumerate(business_cases, 1):
            print(f"\nüß™ TEST {i}/{len(business_cases)}: {case['name']}")
            print("-" * 60)
            print(f"üìù Request: {case['request']}")
            
            try:
                start_time = datetime.now()
                result = await jarvis.process_request(case['request'], f"test_session_{i}")
                duration = (datetime.now() - start_time).total_seconds()
                
                print(f"\n‚è±Ô∏è  Duration: {duration:.2f}s")
                print(f"‚úÖ Success: {result['success']}")
                print(f"üß† Method: {result['method']}")
                print(f"üìä Confidence: {result.get('confidence', 0):.2f}")
                print(f"‚ö° Strategy: {result.get('execution_strategy', 'N/A')}")
                print(f"ü§ñ Agents: {result.get('agents_used', [])}")
                print(f"üéØ Goal: {result.get('business_goal', 'N/A')[:80]}...")
                
                # Validate expectations
                validation_results = []
                
                if 'expected_agent' in case:
                    expected_agent = case['expected_agent']
                    actual_agents = result.get('agents_used', [])
                    if expected_agent in actual_agents:
                        validation_results.append(f"‚úÖ {expected_agent} correctly selected")
                    else:
                        validation_results.append(f"‚ùå Expected {expected_agent}, got {actual_agents}")
                
                if 'expected_strategy' in case:
                    expected_strategy = case['expected_strategy']
                    actual_strategy = result.get('execution_strategy', '')
                    if expected_strategy in actual_strategy:
                        validation_results.append(f"‚úÖ {expected_strategy} strategy used")
                    else:
                        validation_results.append(f"‚ö†Ô∏è  Expected {expected_strategy}, got {actual_strategy}")
                
                if 'expected_agents' in case:
                    expected_agents = case['expected_agents']
                    actual_agents = result.get('agents_used', [])
                    overlap = set(expected_agents) & set(actual_agents)
                    if len(overlap) > 0:
                        validation_results.append(f"‚úÖ {len(overlap)}/{len(expected_agents)} expected agents selected")
                    else:
                        validation_results.append(f"‚ùå None of expected agents selected: {expected_agents}")
                
                # Show validation results
                if validation_results:
                    print(f"\nüîç VALIDATION:")
                    for validation in validation_results:
                        print(f"   {validation}")
                
                # Overall assessment
                confidence = result.get('confidence', 0)
                success = result.get('success', False)
                
                if success and confidence >= 0.7:
                    print(f"‚úÖ TEST PASSED - High quality semantic understanding")
                    results.append(True)
                elif success and confidence >= 0.5:
                    print(f"‚ö†Ô∏è  TEST PARTIAL - Acceptable semantic understanding") 
                    results.append(True)
                else:
                    print(f"‚ùå TEST FAILED - Low confidence or failed execution")
                    results.append(False)
                
            except Exception as e:
                print(f"‚ùå TEST FAILED: {str(e)}")
                results.append(False)
        
        # Final summary
        print(f"\n{'='*60}")
        print("üèÅ BUSINESS USE CASE RESULTS")
        print(f"{'='*60}")
        
        passed = sum(results)
        total = len(results)
        success_rate = passed / total if total > 0 else 0
        
        print(f"üìä Success Rate: {success_rate:.1%} ({passed}/{total})")
        
        for i, (case, result) in enumerate(zip(business_cases, results), 1):
            status = "‚úÖ PASS" if result else "‚ùå FAIL"
            print(f"   {status} {case['name']}")
        
        if success_rate >= 0.75:
            print(f"\nüéâ EXCELLENT! Semantic architecture handles your business cases well")
            print(f"‚úÖ Ready for production with your specific workloads")
        elif success_rate >= 0.5:
            print(f"\nüëç GOOD! Semantic architecture works for most of your cases")
            print(f"‚ö†Ô∏è  Review failed cases and consider tuning")
        else:
            print(f"\n‚ö†Ô∏è  NEEDS WORK! Some business cases need attention")
            print(f"üîß Review implementation and test with mock data first")
        
        return success_rate >= 0.5
        
    except Exception as e:
        print(f"‚ùå SETUP ERROR: {e}")
        return False


async def test_with_mock():
    """Test with mock AI for development."""
    print("ü§ñ Testing with Mock AI")
    print("-" * 30)
    
    from orchestration.semantic_request_parser import SemanticRequestParser
    
    class MockAIEngine:
        async def generate(self, prompt):
            # Simple mock that responds based on keywords
            if "logo" in prompt.lower():
                return '{"business_goal": "Create professional logo", "user_intent_summary": "Logo design needed", "primary_capabilities": ["logo_generation"], "secondary_capabilities": ["brand_creation"], "recommended_agents": ["logo_generation_agent"], "execution_strategy": "single_agent", "execution_plan": {}, "extracted_parameters": {}, "business_context": {}, "user_preferences": {}, "confidence_score": 0.9, "reasoning": "Clear logo request"}'
            elif "market" in prompt.lower() or "research" in prompt.lower():
                return '{"business_goal": "Conduct market research", "user_intent_summary": "Market analysis needed", "primary_capabilities": ["market_analysis"], "secondary_capabilities": [], "recommended_agents": ["market_research_agent"], "execution_strategy": "single_agent", "execution_plan": {}, "extracted_parameters": {}, "business_context": {}, "user_preferences": {}, "confidence_score": 0.85, "reasoning": "Market research request"}'
            else:
                return '{"business_goal": "General business assistance", "user_intent_summary": "General help needed", "primary_capabilities": ["content_creation"], "secondary_capabilities": [], "recommended_agents": ["general_agent"], "execution_strategy": "single_agent", "execution_plan": {}, "extracted_parameters": {}, "business_context": {}, "user_preferences": {}, "confidence_score": 0.7, "reasoning": "General request"}'
    
    parser = SemanticRequestParser(MockAIEngine())
    
    # Test key cases with mock
    test_cases = [
        "Create a logo for my coffee shop",
        "I need market research for electric vehicles",
        "Help me with my business strategy"
    ]
    
    for case in test_cases:
        print(f"\nTesting: {case}")
        understanding = await parser.parse_request(case)
        print(f"  Goal: {understanding.business_goal}")
        print(f"  Agents: {understanding.recommended_agents}")
        print(f"  Confidence: {understanding.confidence_score}")
    
    print(f"\n‚úÖ Mock testing completed - basic functionality working")
    return True


async def quick_validation():
    """Quick validation of your setup."""
    print("‚ö° Quick Validation")
    print("=" * 20)
    
    # Check files exist
    required_files = [
        "orchestration/semantic_request_parser.py",
        "orchestration/semantic_orchestrator.py", 
        "orchestration/semantic_integration.py",
        "orchestration/semantic_universal_orchestrator.py"
    ]
    
    for file in required_files:
        if os.path.exists(file):
            print(f"‚úÖ {file}")
        else:
            print(f"‚ùå {file} - MISSING!")
            return False
    
    # Check API key
    api_key = os.getenv('ANTHROPIC_API_KEY')
    if api_key:
        print(f"‚úÖ API Key loaded: {api_key[:8]}...{api_key[-4:]}")
    else:
        print(f"‚ö†Ô∏è  No API key - will use mock testing")
    
    print(f"\n‚úÖ Setup validation passed")
    return True


if __name__ == "__main__":
    async def main():
        # Quick validation first
        if not await quick_validation():
            print("‚ùå Setup validation failed")
            return False
        
        # Run business case tests
        return await test_your_business_cases()
    
    success = asyncio.run(main())
    
    if success:
        print(f"\nüéâ SUCCESS! Your semantic architecture is working!")
        print(f"üöÄ Ready for production deployment")
    else:
        print(f"\n‚ö†Ô∏è  Some issues detected. Review logs above.")
    
    exit(0 if success else 1)